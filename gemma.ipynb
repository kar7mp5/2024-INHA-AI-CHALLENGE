{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5760c32ac847c982399c1c6deb7a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_8bit_compute_dtype', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e72347f0c5464e89b201bc6832d980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887125212281476ab11bbfd4851c58c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요, 저는 인공지능 모델입니다. 저는 인공지능 모델이기 때문에 제가 하는 말은 모두 사실입니다. 저는 인공지능 모델이기 때문에 제가 하는 말은 모두\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed, BitsAndBytesConfig\n",
    "\n",
    "set_seed(1234)\n",
    "\n",
    "# Identify the checkpoint to use\n",
    "model_checkpoint = \"beomi/gemma-ko-7b\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define the BitsAndBytesConfig for 8-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    bnb_8bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "# Load the model with 8-bit quantization and move it to GPU\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaSdpaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=3072, out_features=4096, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=3072, out_features=4096, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=3072, out_features=4096, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=4096, out_features=3072, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=3072, out_features=24576, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=3072, out_features=24576, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=24576, out_features=3072, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm((3072,), eps=1e-06)\n",
      "        (post_attention_layernorm): GemmaRMSNorm((3072,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm((3072,), eps=1e-06)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
      ")\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "# print out the model layers\n",
    "print(model)\n",
    "print(model.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬 코드 아무거나 짜줘요. 100줄 정도로요. 100줄이면 100줄이라고 말해주세요. 100줄이면 \n"
     ]
    }
   ],
   "source": [
    "# Example text for the model to process\n",
    "text = \"파이썬으로 bfs 짜줘\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=50)\n",
    "\n",
    "# Decode the output to text\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|access_token|>: {access_token}\n",
      "<|refresh_token|>: {refresh_token}\n",
      "<|expires_in|>: {expires_in}\n",
      "<|last_access_date|>: {last_access_date}\n",
      "<|photo_count|>: {photo_count}\n",
      "<|photo_size|>: {photo_size}\n",
      "<|photo_type|>: {photo_type}\n",
      "<|photo_thumb_size|>: {photo_thumb_size}\n",
      "<|photo_title|>: {photo_title}\n",
      "<|photo_description|>: {photo_description}\n",
      "<|photo_date|>: {photo_date}\n",
      "<|photo_thumbnail_url|>: {photo_thumbnail_url}\n",
      "<|photo_original_url|>: {photo_original_url}\n",
      "<|photo_original_size|>: {photo_original_size}\n",
      "<|photo_cropped_size|>: {photo_cropped_size}\n",
      "<|photo_cropped_url|>: {photo_cropped_url}\n",
      "<|photo_cropped_original_size|>: {photo_cropped_original_size}\n",
      "<|photo_cropped_original_url|>: {photo_cropped_original_url}\n",
      "<|photo_cropped_thumbnail_size|>: {photo_cropped_thumbnail_size}\n",
      "<|photo_cropped_thumbnail_url|>: {photo_cropped_thumbnail_url}\n",
      "<|photo_cropped_original_thumbnail_size|>: {photo_cropped_original_thumbnail_size}\n",
      "<|photo_cropped_original_thumbnail_url|>: {photo_cropped_original_thumbnail_url}\n",
      "<|photo_cropped_thumbnail_thumbnail_size|>: {photo_cropped_thumbnail_thumbnail_size}\n",
      "<|photo_cropped_thumbnail_thumbnail_url|>: {photo_cropped_thumbnail_thumbnail_url}\n",
      "<|photo_cropped_original_thumbnail_thumbnail_size|>: {photo_cropped_original_thumbnail_thumbnail_size}\n",
      "<|photo_cropped_original_thumbnail_url|>: {photo_cropped_original_thumbnail_url}\n",
      "<|photo_cropped_thumbnail_thumbnail_original_size|>: {photo_cropped_thumbnail_thumbnail_original\n"
     ]
    }
   ],
   "source": [
    "# create a list of turns for our conversation, each conversation has to start\n",
    "# with a user turn and must alternate between user/model\n",
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Write a hello world program in Python please.\"}\n",
    "]\n",
    "# next we'll use the apply_chat_template from our tokenizer, this will convert\n",
    "# our list into the conversation prompt and at the same time\n",
    "# convert the prompt to tokens all in one call.\n",
    "token_inputs = tokenizer.apply_chat_template(chat, tokenize=True, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "\n",
    "# an important thing to note about the apply_chat_template, it returns the tensor of the input ids directly.\n",
    "# unlike how we called the tokenizer before and got back a dict with the input_ids entry,\n",
    "# it's not a huge change, just need to keep in mind this returns a slightly different structure.\n",
    "\n",
    "# call inference on the model\n",
    "token_outputs = model.generate(input_ids=token_inputs, do_sample=True, max_new_tokens=500, temperature=.5)\n",
    "new_tokens = token_outputs[0][token_inputs.shape[-1]:]\n",
    "decoded_output = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
